
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>h2_q3</title><meta name="generator" content="MATLAB 9.4"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2019-02-19"><meta name="DC.source" content="h2_q3.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><p><b>Homework 2 - Question 3</b></p><pre class="codeinput">data_1 = csvread(<span class="string">'brca_reduced.csv'</span>);
Y_1 = data_1(:, end);
X_1 = data_1(:, 1:end-1);

<span class="comment">%a)</span>
disp(<span class="string">"3a)"</span>)
fprintf(<span class="string">"OLS fit \n"</span>)
X_1 = [ones(size(Y_1)),X_1];
B_ols_1 = regress(Y_1,X_1);
MSE_ols_1 = mean((Y_1 - X_1*B_ols_1).^2);
fprintf(<span class="string">"MSE for clean data = %f \n"</span>,MSE_ols_1);


data_2 = csvread(<span class="string">'brca_noisy.csv'</span>);
Y_2 = data_2(:, end);
X_2 = data_2(:, 1:end-1);
X_2 = [ones(size(Y_2)),X_2];
B_ols_2 = regress(Y_2,X_2);
MSE_ols_2 = mean((Y_2 - X_2*B_ols_2).^2);
fprintf(<span class="string">"MSE for noisy data = %f \n"</span>, MSE_ols_2)
norm_ols = norm(B_ols_2-B_ols_1, inf);
fprintf(<span class="string">"l-inf norm for noisy data = %f \n"</span>,norm_ols)

disp(<span class="string">" "</span>)
<span class="comment">%b)</span>
disp(<span class="string">"3b)"</span>)
fprintf(<span class="string">"Robust fit using huber loss \n"</span>)
X_1 = data_1(:, 1:end-1);
B_robust_1 = robustfit(X_1,Y_1, <span class="string">'huber'</span>);
MSE_robust_1 = mean((Y_1 - (X_1*B_robust_1(2:end) + B_robust_1(1))).^2);
fprintf(<span class="string">"MSE for clean data= %f \n"</span>,MSE_robust_1)

X_2 = data_2(:, 1:end-1);
B_robust_2 = robustfit(X_2,Y_2, <span class="string">'huber'</span>);
MSE_robust_2 = mean((Y_2 - (X_2*B_robust_2(2:end) + B_robust_2(1))).^2);
fprintf(<span class="string">"MSE for noisy data = %f \n"</span>,MSE_robust_2)
norm_robost = norm(B_robust_2-B_robust_1, inf);
fprintf(<span class="string">"l-inf norm = %f \n"</span>,norm_robost)
disp(<span class="string">"On comparing a) and b) MSE of huber is more than OLS and huber results to more sparse coefficient matrix."</span>);
disp(<span class="string">" "</span>)
<span class="comment">%c)</span>
disp(<span class="string">"3c)"</span>)
losses = [<span class="string">"cauchy"</span>, <span class="string">"talwar"</span>, <span class="string">"welsch"</span>];
<span class="keyword">for</span> i =1:length(losses)
    loss = losses(i);
    fprintf(<span class="string">"Robust fit using %s loss \n"</span>, loss)
    B_robust_1 = robustfit(X_1,Y_1, loss);
    MSE_robust_1 = mean((Y_1 - (X_1*B_robust_1(2:end) + B_robust_1(1))).^2);
    fprintf(<span class="string">"MSE for clean data = %f \n"</span>,MSE_robust_1);

    X_2 = data_2(:, 1:end-1);
    B_robust_2 = robustfit(X_2,Y_2, loss);
    MSE_robust_2 = mean((Y_2 - (X_2*B_robust_2(2:end) + B_robust_2(1))).^2);
    fprintf(<span class="string">"MSE for noisy data = %f \n"</span>,MSE_robust_2)
    norm_robost = norm(B_robust_2-B_robust_1, inf);
    fprintf(<span class="string">"l-inf norm = %f \n"</span>,norm_robost)
    disp(<span class="string">" "</span>)
<span class="keyword">end</span>


disp(<span class="string">"We can see that in all the cases &#8211; OLS and Robust Regression, MSE calcuated for clean data is much lower than that calculated for noisy data"</span>);
disp(<span class="string">"MSE  for robust regression(in all 4 cases) is more than that of OLS for both noisy and clean data. The l-&#8734; norm of the difference between the"</span>);
disp(<span class="string">"regression coefficients, is much more in case of OLS. This shows that OLS is highly effected to outliers when compared to robust regression."</span>)
disp(<span class="string">"Specifically for Cauchy, Talwar and Welsch loss, MSE is higher than Huber and same goes with sparsity of coefficients. This could indicate "</span>);
disp(<span class="string">"they are more robust when compared to Huber"</span>);
</pre><pre class="codeoutput">3a)
OLS fit 
MSE for clean data = 0.159327 
MSE for noisy data = 10.422337 
l-inf norm for noisy data = 2.239375 
 
3b)
Robust fit using huber loss 
MSE for clean data= 0.167658 
MSE for noisy data = 13.604143 
l-inf norm = 0.221294 
On comparing a) and b) MSE of huber is more than OLS and huber results to more sparse coefficient matrix.
 
3c)
Robust fit using cauchy loss 
MSE for clean data = 0.170836 
MSE for noisy data = 14.490056 
l-inf norm = 0.176621 
 
Robust fit using talwar loss 
MSE for clean data = 0.173959 
MSE for noisy data = 14.742861 
l-inf norm = 0.187621 
 
Robust fit using welsch loss 
MSE for clean data = 0.182239 
MSE for noisy data = 14.698762 
l-inf norm = 0.192031 
 
We can see that in all the cases &#8211; OLS and Robust Regression, MSE calcuated for clean data is much lower than that calculated for noisy data
MSE  for robust regression(in all 4 cases) is more than that of OLS for both noisy and clean data. The l-&#8734; norm of the difference between the
regression coefficients, is much more in case of OLS. This shows that OLS is highly effected to outliers when compared to robust regression.
Specifically for Cauchy, Talwar and Welsch loss, MSE is higher than Huber and same goes with sparsity of coefficients. This could indicate 
they are more robust when compared to Huber
</pre><p class="footer"><br><a href="https://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2018a</a><br></p></div><!--
##### SOURCE BEGIN #####
%%
% *Homework 2 - Question 3*
data_1 = csvread('brca_reduced.csv');
Y_1 = data_1(:, end);
X_1 = data_1(:, 1:end-1);

%a)
disp("3a)")
fprintf("OLS fit \n")
X_1 = [ones(size(Y_1)),X_1];
B_ols_1 = regress(Y_1,X_1);
MSE_ols_1 = mean((Y_1 - X_1*B_ols_1).^2);
fprintf("MSE for clean data = %f \n",MSE_ols_1);


data_2 = csvread('brca_noisy.csv');
Y_2 = data_2(:, end);
X_2 = data_2(:, 1:end-1);
X_2 = [ones(size(Y_2)),X_2];
B_ols_2 = regress(Y_2,X_2);
MSE_ols_2 = mean((Y_2 - X_2*B_ols_2).^2);
fprintf("MSE for noisy data = %f \n", MSE_ols_2)
norm_ols = norm(B_ols_2-B_ols_1, inf);
fprintf("l-inf norm for noisy data = %f \n",norm_ols)

disp(" ")
%b)
disp("3b)")
fprintf("Robust fit using huber loss \n")
X_1 = data_1(:, 1:end-1);
B_robust_1 = robustfit(X_1,Y_1, 'huber');
MSE_robust_1 = mean((Y_1 - (X_1*B_robust_1(2:end) + B_robust_1(1))).^2);
fprintf("MSE for clean data= %f \n",MSE_robust_1)

X_2 = data_2(:, 1:end-1);
B_robust_2 = robustfit(X_2,Y_2, 'huber');
MSE_robust_2 = mean((Y_2 - (X_2*B_robust_2(2:end) + B_robust_2(1))).^2);
fprintf("MSE for noisy data = %f \n",MSE_robust_2)
norm_robost = norm(B_robust_2-B_robust_1, inf);
fprintf("l-inf norm = %f \n",norm_robost)
disp("On comparing a) and b) MSE of huber is more than OLS and huber results to more sparse coefficient matrix.");
disp(" ")
%c)
disp("3c)")
losses = ["cauchy", "talwar", "welsch"];
for i =1:length(losses)
    loss = losses(i);
    fprintf("Robust fit using %s loss \n", loss)
    B_robust_1 = robustfit(X_1,Y_1, loss);
    MSE_robust_1 = mean((Y_1 - (X_1*B_robust_1(2:end) + B_robust_1(1))).^2);
    fprintf("MSE for clean data = %f \n",MSE_robust_1);
    
    X_2 = data_2(:, 1:end-1);
    B_robust_2 = robustfit(X_2,Y_2, loss);
    MSE_robust_2 = mean((Y_2 - (X_2*B_robust_2(2:end) + B_robust_2(1))).^2);
    fprintf("MSE for noisy data = %f \n",MSE_robust_2)
    norm_robost = norm(B_robust_2-B_robust_1, inf);
    fprintf("l-inf norm = %f \n",norm_robost)
    disp(" ")
end


disp("We can see that in all the cases – OLS and Robust Regression, MSE calcuated for clean data is much lower than that calculated for noisy data");
disp("MSE  for robust regression(in all 4 cases) is more than that of OLS for both noisy and clean data. The l-∞ norm of the difference between the");
disp("regression coefficients, is much more in case of OLS. This shows that OLS is highly effected to outliers when compared to robust regression.") 
disp("Specifically for Cauchy, Talwar and Welsch loss, MSE is higher than Huber and same goes with sparsity of coefficients. This could indicate ");
disp("they are more robust when compared to Huber");







##### SOURCE END #####
--></body></html>